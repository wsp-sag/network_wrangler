name: Push Workflow

on: [push]

jobs:
  build:

    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9]

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    - name: Document branch
      run: echo ${{ github.ref_name }}
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    - name: Configure Git user
      run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements.tests.txt
        pip install -r requirements.docs.txt
    - name: Install package
      run: |
        pip install -e .[test]
    - name: Run Ruff
      run: ruff check --output-format=github network_wrangler
    - name: Run tests with coverage and benchmarking
      run: |
          set -o pipefail
          pytest --junitxml=pytest.xml \
                 --benchmark-save=pr_benchmark \
                 --benchmark-json=pr_benchmark.json \
                 --cov-report=term-missing:skip-covered \
                 --cov=network_wrangler tests/ | \
          tee pytest-coverage.txt
    - name: Pytest coverage comment
      if: github.event_name == 'pull_request'
      uses: MishaKav/pytest-coverage-comment@main
      with:
        pytest-coverage-path: ./pytest-coverage.txt
        junitxml-path: ./pytest.xml
    - name: If a pull request, add a comment about coverage
      if: github.event_name == 'pull_request'
      uses: coroo/pytest-coverage-commentator@v1.0.2
    - name: Retrieve benchmark results from base branch
      if: github.event_name == 'pull_request'
      id: compare
      run: |
        git checkout ${{ github.event.pull_request.base.ref }}
        if [ -f "tests/.benchmarks/benchmark_results.json" ]; then
          cp tests/.benchmarks/benchmark_results.json base_benchmark.json
          echo "::set-output name=benchmark_exists::true"
        else
          echo "::set-output name=benchmark_exists::false"
        fi
    - name: Compare benchmarks
      if: steps.compare.outputs.benchmark_exists == 'true'
      run: |
        pytest-benchmark compare base_benchmark.json pr_benchmark.json --csv=comparison.csv --json=comparison.json --histogram=histogram.png

    - name: Post comparison comment on pull request
      if: steps.compare.outputs.benchmark_exists == 'true'
      uses: marocchino/sticky-pull-request-comment@v2
      with:
        header: 'Benchmark Comparison'
        message: |
          ## Benchmark Results
          ![Benchmark Comparison](./histogram.png)

          | Test name | PR Benchmark | Base Benchmark | Difference |
          | --------- | ------------ | -------------- | ---------- |
          ```bash
          pytest-benchmark compare base_benchmark.json pr_benchmark.json --csv --diff
          ```
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
